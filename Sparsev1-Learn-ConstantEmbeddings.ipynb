{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###el bueno\n",
    "import scipy.io\n",
    "import numpy\n",
    "\n",
    "import torchtext\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##DATA\n",
    "#predicates: is_a, has_a\n",
    "#subjects: (animal,bird, fish, canary, eagle, shark, salmon)\n",
    "#objects: (breathes, can fly, can swim, can sing, has claws, can bite, is pink)\n",
    "data = torch.Tensor(2,7,7).zero_()\n",
    "##is_a\n",
    "data[0,:,:7] = torch.eye(7)\n",
    "data[0,:,0] = torch.ones(7,1)\n",
    "data[0,3,1], data[0,4,1], data[0,5,2], data[0,6,2] = 1,1,1,1 \n",
    "##has_a\n",
    "data[1,:,0] = torch.ones(1,7)\n",
    "data[1,1,1], data[1,2,2] = 1,1\n",
    "data[1,3:5,1] = torch.ones(2,1) \n",
    "data[1,5:,2] = torch.ones(2,1)\n",
    "data[1,3:,3:] = torch.eye(4)\n",
    "\n",
    "##For Sparse\n",
    "#data[0,6,0],data[1,6,0],data[1,6,2],data[1,6,6] = 0,0,0,0  #It knows [0,6,2] ... salmon is fish\n",
    "#data[0,6,0],data[1,6,0],data[0,6,2],data[1,6,6] = 0,0,0,0  #It knows [1,6,2] ... salmon can swim\n",
    "\n",
    "#data[0,6,0],data[0,6,2],data[1,6,6] = 0,0,0  #It knows [1,6,2],[1,6,0] ... salmon can swim and breathe\n",
    "\n",
    "##Auxiliar, very small\n",
    "#data = torch.Tensor(2,2,2)\n",
    "#data[0,:,:] = torch.eye(2,2)\n",
    "#data[0,1,0] = 1\n",
    "#data[1,:,0]= torch.Tensor([1,1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#num_predicates = data.size()[0]\n",
    "#num_subjects = data.size()[1]\n",
    "#num_objects = data.size()[2]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "    1     0     1  ...      0     0     0\n",
      "    1     0     0  ...      0     0     0\n",
      "    1     0     0  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     1     0  ...      0     0     0\n",
      "    0     1     0  ...      0     0     0\n",
      "    0     1     0  ...      0     0     1\n",
      "[torch.FloatTensor of size 34x30]\n",
      "\n",
      "Variable containing:\n",
      "    1     0     1  ...      0     0     0\n",
      "    1     0     1  ...      0     0     0\n",
      "    1     0     1  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    0     1     0  ...      0     0     0\n",
      "    0     1     0  ...      1     0     0\n",
      "    0     1     0  ...      0     1     0\n",
      "[torch.FloatTensor of size 64x30]\n",
      "\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "num_objects = 7\n",
    "num_subjects = 7\n",
    "num_constants = num_objects + num_subjects\n",
    "num_predicates = 2\n",
    "threshold = .1\n",
    "\n",
    "constants = torch.eye(num_constants)\n",
    "predicates = torch.eye(num_predicates)\n",
    "\n",
    "num_feat_facts = predicates.size()[1] + 2*constants.size()[1]\n",
    "\n",
    "knowledge_pos = torch.zeros([1, num_feat_facts])\n",
    "knowledge_neg = torch.zeros([1, num_feat_facts])\n",
    "\n",
    "for predicate in range(num_predicates):\n",
    "    for obj in range(num_objects):\n",
    "        for subj in range(num_constants):\n",
    "            fact = torch.cat((predicates[predicate].view(1,-1), constants[obj].view(1,-1), constants[subj].view(1,-1)), 1)\n",
    "            if (predicate == 0  and subj<7) or (predicate == 1 and subj>=7):\n",
    "                if data[predicate, obj, subj%7] == 1:\n",
    "                    knowledge_pos = torch.cat((knowledge_pos, fact) , 0)\n",
    "                else:\n",
    "                    knowledge_neg = torch.cat((knowledge_neg, fact) , 0)\n",
    "                \n",
    "knowledge_pos = Variable(knowledge_pos.narrow(0, 1, knowledge_pos.size()[0]-1))\n",
    "knowledge_neg = Variable(knowledge_neg.narrow(0, 1, knowledge_neg.size()[0]-1))\n",
    "\n",
    "print(knowledge_pos)\n",
    "print(knowledge_neg)\n",
    "print(num_feat_facts)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read(state):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_step(facts):\n",
    "    new_facts = facts.clone()\n",
    "    for rule in rules:\n",
    "        for fact1 in facts:\n",
    "            for fact2 in facts:\n",
    "                p = fact1[-1]*fact2[-1]\n",
    "                #pdb.set_trace()\n",
    "                p = p*F.cosine_similarity(rule[num_predicates:2*num_predicates].view(1,-1), fact1[:num_predicates].view(1,-1))\n",
    "                p = p*F.cosine_similarity(rule[2*num_predicates:3*num_predicates].view(1,-1), fact2[:num_predicates].view(1,-1))\n",
    "                p = p*F.cosine_similarity(fact1[num_predicates+num_constants:-1].view(1,-1) , fact2[num_predicates:num_predicates+num_constants].view(1,-1))\n",
    "                new_fact = torch.cat((rule[:num_predicates], fact1[num_predicates:num_predicates+num_constants], \\\n",
    "                                                              fact2[num_predicates+num_constants:-1], p), 0)\n",
    "                new_facts = torch.cat(( new_facts, new_fact.view(1,-1) ),0)\n",
    "    _ , index = torch.topk(new_facts[:,-1], K)\n",
    "    new_facts = torch.index_select(new_facts, 0, index)\n",
    "    #pdb.set_trace()            \n",
    "    #new_facts = torch.topk(new_facts, K, dim = new_facts.size()[1])            \n",
    "    return new_facts\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'losssssssssssssssssssss', 311.6467590332031)\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9720  0.6026  0.2250  0.8916  0.0245  0.0110  0.3445  0.3596  0.8250  0.7337\n",
      " 0.3218  0.4864  0.1511  0.6014  0.4471  0.9201  0.7428  0.9012  0.7616  0.4222\n",
      " 0.7766  0.8725  0.4035  0.5884  0.3577  0.6154  0.0189  0.6246  0.4478  0.6360\n",
      " 0.1685  0.8511  0.0769  0.3039  0.3830  0.0282  0.0390  0.1836  0.3260  0.0795\n",
      " 0.3588  0.3617  0.4887  0.0902  0.9453  0.2273  0.5861  0.6861  0.0789  0.4465\n",
      " 0.6395  0.3428  0.1281  0.3027  0.8186  0.4661  0.2731  0.3000  0.2066  0.7375\n",
      " 0.2702  0.7410  0.7132  0.6030  0.7350  0.9627  0.1558  0.6939  0.7730  0.9542\n",
      " 0.4793  0.5039  0.4560  0.4809  0.2189  0.4290  0.2503  0.6937  0.4838  0.1885\n",
      " 0.2231  0.7272  0.0677  0.4816  0.1219  0.9006  0.1483  0.9157  0.8097  0.9851\n",
      " 0.0067  0.0762  0.8571  0.5806  0.7613  0.2302  0.5681  0.7362  0.4576  0.0023\n",
      " 0.3276  0.6331  0.2733  0.5590  0.9681  0.5829  0.1800  0.3057  0.8348  0.8976\n",
      " 0.0606  0.7968  0.4716  0.9523  0.2627  0.0205  0.3589  0.2936  0.7102  0.8064\n",
      " 0.0915  0.8766  0.2080  0.3466  0.1126  0.0319  0.0663  0.2388  0.6130  0.7098\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.1362  0.7604  0.2317  0.7868  0.5087  0.0945  0.6256  0.6374  0.1337  0.2209\n",
      " 0.1182  0.1565  0.6081  0.6193  0.2567  0.2139  0.2937  0.4365  0.7564  0.6760\n",
      " 0.1517  0.9860  0.5358  0.6949  0.5328  0.8999  0.7781  0.8334  0.1964  0.5020\n",
      " 0.2719  0.2173  0.1657  0.9213  0.2948  0.8988  0.1843  0.4446  0.7764  0.3211\n",
      " 0.2820  0.8109  0.3352  0.4565  0.7546  0.5771  0.5123  0.1555  0.4283  0.9801\n",
      " 0.7385  0.4400  0.1091  0.6184  0.3043  0.4834  0.7634  0.8504  0.4358  0.7567\n",
      " 0.0948  0.6547  0.4841  0.2267  0.8835  0.9260  0.1454  0.6800  0.6435  0.6754\n",
      " 0.8833  0.2309  0.3376  0.6739  0.0385  0.7980  0.3269  0.8721  0.0131  0.6023\n",
      " 0.0959  0.3250  0.9261  0.1372  0.7229  0.4221  0.2260  0.4303  0.5442  0.1116\n",
      " 0.2438  0.3477  0.5966  0.6265  0.6459  0.6310  0.5051  0.2811  0.2212  0.2334\n",
      " 0.5284  0.8930  0.7529  0.0736  0.1761  0.9523  0.4898  0.8754  0.1734  0.8949\n",
      " 0.7665  0.4351  0.4690  0.4643  0.8572  0.2761  0.7792  0.7324  0.8784  0.8359\n",
      " 0.3552  0.6753  0.5559  0.8766  0.5109  0.4983  0.5461  0.6978  0.6829  0.6735\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.3227  0.9296  0.7629  0.4138  0.5740  0.3193  0.9180  0.6566  0.5427  0.5753\n",
      " 0.0119  0.5439  0.0329  0.1400  0.7730  0.0068  0.8581  0.5653  0.5175  0.5094\n",
      " 0.9414  0.0748  0.6914  0.4235  0.1134  0.7513  0.3472  0.9030  0.8440  0.8851\n",
      " 0.3546  0.3895  0.6302  0.4967  0.9626  0.2741  0.5309  0.6824  0.3958  0.7106\n",
      " 0.1529  0.2453  0.3069  0.9838  0.6387  0.1010  0.2598  0.1555  0.8988  0.3587\n",
      " 0.6602  0.1170  0.1046  0.4094  0.9975  0.0845  0.7874  0.2300  0.1550  0.1607\n",
      " 0.0770  0.1444  0.3745  0.7024  0.2569  0.4930  0.4262  0.1263  0.3239  0.9897\n",
      " 0.9705  0.0693  0.3041  0.4372  0.7654  0.2526  0.5793  0.8637  0.2463  0.7958\n",
      " 0.7239  0.6882  0.0194  0.8443  0.7429  0.4080  0.4175  0.5186  0.0415  0.1131\n",
      " 0.7295  0.7900  0.4069  0.0485  0.4364  0.7160  0.7926  0.3681  0.0719  0.1435\n",
      " 0.7115  0.0024  0.6121  0.8772  0.9820  0.3676  0.7430  0.2390  0.3414  0.3223\n",
      " 0.4742  0.3563  0.5791  0.9684  0.3434  0.1744  0.1243  0.9450  0.9790  0.6855\n",
      " 0.7488  0.5134  0.5628  0.7034  0.5308  0.7215  0.0086  0.8369  0.1436  0.2992\n",
      "[torch.FloatTensor of size 13x30]\n",
      "\n",
      "(1, 'losssssssssssssssssssss', 313.82501220703125)\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9730  0.6016  0.2240  0.8906  0.0255  0.0120  0.3435  0.3586  0.8240  0.7327\n",
      " 0.3228  0.4854  0.1501  0.6004  0.4461  0.9211  0.7418  0.9002  0.7606  0.4212\n",
      " 0.7776  0.8715  0.4025  0.5874  0.3567  0.6144  0.0199  0.6236  0.4468  0.6350\n",
      " 0.1695  0.8501  0.0759  0.3029  0.3820  0.0292  0.0380  0.1826  0.3250  0.0785\n",
      " 0.3598  0.3607  0.4877  0.0912  0.9443  0.2263  0.5851  0.6851  0.0779  0.4455\n",
      " 0.6405  0.3418  0.1271  0.3017  0.8176  0.4651  0.2741  0.2990  0.2056  0.7365\n",
      " 0.2712  0.7400  0.7122  0.6020  0.7360  0.9617  0.1548  0.6929  0.7720  0.9532\n",
      " 0.4803  0.5029  0.4550  0.4799  0.2179  0.4280  0.2513  0.6927  0.4828  0.1875\n",
      " 0.2241  0.7262  0.0667  0.4806  0.1209  0.8996  0.1473  0.9167  0.8087  0.9841\n",
      " 0.0077  0.0752  0.8561  0.5796  0.7603  0.2292  0.5671  0.7372  0.4566  0.0013\n",
      " 0.3286  0.6321  0.2723  0.5580  0.9671  0.5839  0.1790  0.3047  0.8338  0.8966\n",
      " 0.0616  0.7958  0.4706  0.9513  0.2617  0.0195  0.3599  0.2926  0.7092  0.8054\n",
      " 0.0925  0.8756  0.2090  0.3456  0.1116  0.0309  0.0653  0.2378  0.6120  0.7088\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.1352  0.7594  0.2307  0.7858  0.5077  0.0935  0.6247  0.6364  0.1327  0.2199\n",
      " 0.1172  0.1555  0.6071  0.6183  0.2557  0.2129  0.2947  0.4355  0.7554  0.6750\n",
      " 0.1507  0.9850  0.5348  0.6939  0.5318  0.8989  0.7791  0.8324  0.1954  0.5010\n",
      " 0.2709  0.2163  0.1647  0.9203  0.2938  0.8978  0.1833  0.4436  0.7754  0.3201\n",
      " 0.2810  0.8099  0.3342  0.4555  0.7536  0.5761  0.5133  0.1545  0.4273  0.9791\n",
      " 0.7375  0.4390  0.1081  0.6174  0.3033  0.4824  0.7624  0.8514  0.4348  0.7557\n",
      " 0.0938  0.6537  0.4831  0.2257  0.8825  0.9250  0.1444  0.6790  0.6425  0.6744\n",
      " 0.8823  0.2299  0.3366  0.6729  0.0375  0.7970  0.3259  0.8731  0.0121  0.6013\n",
      " 0.0949  0.3240  0.9251  0.1362  0.7219  0.4211  0.2270  0.4293  0.5432  0.1106\n",
      " 0.2428  0.3467  0.5956  0.6255  0.6449  0.6300  0.5041  0.2801  0.2202  0.2324\n",
      " 0.5274  0.8920  0.7519  0.0726  0.1751  0.9513  0.4889  0.8744  0.1724  0.8939\n",
      " 0.7655  0.4341  0.4680  0.4633  0.8562  0.2751  0.7782  0.7314  0.8774  0.8349\n",
      " 0.3542  0.6743  0.5549  0.8756  0.5099  0.4973  0.5451  0.6968  0.6819  0.6725\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.3218  0.9286  0.7619  0.4130  0.5731  0.3187  0.9170  0.6556  0.5417  0.5745\n",
      " 0.0109  0.5429  0.0319  0.1390  0.7720  0.0078  0.8571  0.5643  0.5165  0.5084\n",
      " 0.9404  0.0738  0.6904  0.4225  0.1124  0.7503  0.3462  0.9020  0.8430  0.8841\n",
      " 0.3536  0.3885  0.6292  0.4957  0.9616  0.2731  0.5299  0.6814  0.3948  0.7096\n",
      " 0.1519  0.2443  0.3059  0.9828  0.6377  0.1000  0.2588  0.1545  0.8978  0.3577\n",
      " 0.6592  0.1160  0.1036  0.4084  0.9965  0.0835  0.7864  0.2290  0.1540  0.1597\n",
      " 0.0760  0.1434  0.3735  0.7014  0.2579  0.4920  0.4252  0.1253  0.3229  0.9887\n",
      " 0.9695  0.0683  0.3031  0.4362  0.7644  0.2516  0.5783  0.8627  0.2453  0.7948\n",
      " 0.7229  0.6872  0.0184  0.8433  0.7419  0.4070  0.4165  0.5176  0.0405  0.1121\n",
      " 0.7285  0.7890  0.4059  0.0495  0.4354  0.7150  0.7916  0.3671  0.0709  0.1425\n",
      " 0.7105  0.0015  0.6111  0.8762  0.9810  0.3668  0.7420  0.2381  0.3404  0.3219\n",
      " 0.4732  0.3553  0.5781  0.9674  0.3424  0.1754  0.1233  0.9440  0.9780  0.6845\n",
      " 0.7478  0.5124  0.5618  0.7024  0.5298  0.7205  0.0076  0.8359  0.1426  0.2982\n",
      "[torch.FloatTensor of size 13x30]\n",
      "\n",
      "(2, 'losssssssssssssssssssss', 311.7046813964844)\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9740  0.6006  0.2230  0.8896  0.0264  0.0130  0.3425  0.3576  0.8230  0.7317\n",
      " 0.3238  0.4844  0.1491  0.5994  0.4451  0.9221  0.7408  0.8992  0.7596  0.4202\n",
      " 0.7786  0.8705  0.4015  0.5864  0.3557  0.6134  0.0207  0.6231  0.4458  0.6340\n",
      " 0.1705  0.8491  0.0749  0.3019  0.3810  0.0299  0.0370  0.1830  0.3240  0.0775\n",
      " 0.3608  0.3597  0.4867  0.0920  0.9433  0.2253  0.5847  0.6841  0.0769  0.4445\n",
      " 0.6415  0.3408  0.1261  0.3007  0.8166  0.4641  0.2751  0.2980  0.2046  0.7355\n",
      " 0.2722  0.7390  0.7112  0.6010  0.7358  0.9609  0.1538  0.6919  0.7710  0.9522\n",
      " 0.4813  0.5019  0.4540  0.4797  0.2169  0.4270  0.2519  0.6917  0.4818  0.1865\n",
      " 0.2251  0.7252  0.0657  0.4796  0.1199  0.8989  0.1463  0.9162  0.8077  0.9831\n",
      " 0.0087  0.0742  0.8551  0.5786  0.7593  0.2282  0.5667  0.7370  0.4556  0.0004\n",
      " 0.3296  0.6311  0.2713  0.5570  0.9661  0.5840  0.1794  0.3037  0.8328  0.8956\n",
      " 0.0626  0.7948  0.4696  0.9503  0.2619  0.0185  0.3604  0.2916  0.7082  0.8044\n",
      " 0.0935  0.8746  0.2097  0.3446  0.1121  0.0299  0.0643  0.2368  0.6110  0.7078\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.1342  0.7584  0.2297  0.7848  0.5067  0.0925  0.6238  0.6354  0.1317  0.2189\n",
      " 0.1162  0.1545  0.6061  0.6173  0.2547  0.2119  0.2942  0.4347  0.7544  0.6740\n",
      " 0.1497  0.9840  0.5338  0.6929  0.5308  0.8979  0.7788  0.8314  0.1957  0.5000\n",
      " 0.2699  0.2153  0.1637  0.9193  0.2928  0.8968  0.1829  0.4427  0.7744  0.3191\n",
      " 0.2800  0.8089  0.3332  0.4545  0.7526  0.5751  0.5143  0.1535  0.4263  0.9781\n",
      " 0.7365  0.4380  0.1071  0.6164  0.3023  0.4814  0.7618  0.8510  0.4338  0.7547\n",
      " 0.0928  0.6527  0.4821  0.2247  0.8815  0.9240  0.1434  0.6780  0.6416  0.6735\n",
      " 0.8813  0.2289  0.3356  0.6719  0.0365  0.7960  0.3249  0.8741  0.0111  0.6003\n",
      " 0.0939  0.3230  0.9241  0.1352  0.7209  0.4201  0.2280  0.4283  0.5422  0.1096\n",
      " 0.2418  0.3457  0.5946  0.6245  0.6439  0.6290  0.5031  0.2792  0.2195  0.2314\n",
      " 0.5264  0.8910  0.7509  0.0716  0.1741  0.9503  0.4879  0.8734  0.1714  0.8930\n",
      " 0.7645  0.4331  0.4670  0.4623  0.8552  0.2741  0.7772  0.7304  0.8764  0.8339\n",
      " 0.3532  0.6733  0.5539  0.8746  0.5089  0.4963  0.5441  0.6958  0.6809  0.6715\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.3208  0.9276  0.7609  0.4122  0.5721  0.3183  0.9160  0.6546  0.5407  0.5736\n",
      " 0.0099  0.5419  0.0309  0.1380  0.7710  0.0088  0.8561  0.5633  0.5155  0.5074\n",
      " 0.9394  0.0728  0.6894  0.4215  0.1114  0.7493  0.3452  0.9010  0.8420  0.8831\n",
      " 0.3526  0.3875  0.6282  0.4947  0.9606  0.2721  0.5289  0.6804  0.3938  0.7086\n",
      " 0.1509  0.2433  0.3049  0.9818  0.6367  0.0990  0.2578  0.1535  0.8968  0.3567\n",
      " 0.6582  0.1150  0.1026  0.4074  0.9955  0.0825  0.7854  0.2280  0.1530  0.1587\n",
      " 0.0750  0.1424  0.3725  0.7004  0.2589  0.4910  0.4242  0.1243  0.3219  0.9877\n",
      " 0.9685  0.0673  0.3021  0.4352  0.7634  0.2506  0.5773  0.8617  0.2443  0.7938\n",
      " 0.7219  0.6862  0.0174  0.8423  0.7409  0.4060  0.4155  0.5166  0.0395  0.1111\n",
      " 0.7275  0.7880  0.4049  0.0505  0.4344  0.7140  0.7906  0.3661  0.0699  0.1415\n",
      " 0.7096  0.0007  0.6102  0.8752  0.9801  0.3658  0.7410  0.2371  0.3395  0.3219\n",
      " 0.4723  0.3543  0.5771  0.9664  0.3414  0.1764  0.1223  0.9430  0.9770  0.6835\n",
      " 0.7468  0.5114  0.5608  0.7014  0.5288  0.7195  0.0066  0.8349  0.1416  0.2972\n",
      "[torch.FloatTensor of size 13x30]\n",
      "\n",
      "(3, 'losssssssssssssssssssss', 313.20025634765625)\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9750  0.5996  0.2221  0.8886  0.0273  0.0140  0.3415  0.3567  0.8220  0.7307\n",
      " 0.3248  0.4834  0.1481  0.5988  0.4441  0.9216  0.7398  0.8982  0.7586  0.4192\n",
      " 0.7796  0.8695  0.4005  0.5854  0.3547  0.6124  0.0213  0.6230  0.4448  0.6330\n",
      " 0.1715  0.8481  0.0739  0.3009  0.3800  0.0308  0.0360  0.1832  0.3230  0.0765\n",
      " 0.3618  0.3587  0.4857  0.0925  0.9425  0.2243  0.5841  0.6831  0.0759  0.4435\n",
      " 0.6425  0.3398  0.1251  0.2997  0.8157  0.4631  0.2761  0.2970  0.2036  0.7345\n",
      " 0.2732  0.7380  0.7106  0.6000  0.7353  0.9601  0.1528  0.6909  0.7700  0.9512\n",
      " 0.4823  0.5009  0.4530  0.4799  0.2159  0.4260  0.2523  0.6907  0.4808  0.1855\n",
      " 0.2261  0.7242  0.0647  0.4786  0.1189  0.8981  0.1466  0.9154  0.8067  0.9821\n",
      " 0.0097  0.0732  0.8541  0.5776  0.7583  0.2283  0.5662  0.7365  0.4546 -0.0004\n",
      " 0.3306  0.6301  0.2704  0.5560  0.9651  0.5843  0.1796  0.3027  0.8318  0.8946\n",
      " 0.0636  0.7938  0.4686  0.9493  0.2619  0.0175  0.3610  0.2906  0.7072  0.8034\n",
      " 0.0945  0.8736  0.2101  0.3436  0.1124  0.0289  0.0648  0.2358  0.6100  0.7068\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.1332  0.7574  0.2287  0.7838  0.5057  0.0915  0.6228  0.6344  0.1315  0.2179\n",
      " 0.1152  0.1535  0.6051  0.6163  0.2537  0.2109  0.2936  0.4339  0.7534  0.6730\n",
      " 0.1487  0.9830  0.5328  0.6919  0.5298  0.8969  0.7787  0.8304  0.1959  0.4990\n",
      " 0.2689  0.2143  0.1627  0.9183  0.2918  0.8958  0.1824  0.4418  0.7734  0.3183\n",
      " 0.2790  0.8079  0.3322  0.4535  0.7516  0.5741  0.5147  0.1525  0.4261  0.9771\n",
      " 0.7355  0.4370  0.1061  0.6154  0.3013  0.4804  0.7614  0.8503  0.4328  0.7537\n",
      " 0.0918  0.6517  0.4811  0.2237  0.8805  0.9230  0.1431  0.6770  0.6406  0.6725\n",
      " 0.8803  0.2279  0.3346  0.6709  0.0355  0.7950  0.3248  0.8737  0.0101  0.5993\n",
      " 0.0929  0.3220  0.9231  0.1342  0.7199  0.4191  0.2290  0.4273  0.5412  0.1086\n",
      " 0.2408  0.3447  0.5936  0.6235  0.6429  0.6280  0.5022  0.2782  0.2187  0.2304\n",
      " 0.5254  0.8900  0.7499  0.0706  0.1731  0.9493  0.4869  0.8725  0.1705  0.8920\n",
      " 0.7635  0.4321  0.4660  0.4613  0.8542  0.2731  0.7762  0.7294  0.8754  0.8329\n",
      " 0.3522  0.6723  0.5529  0.8736  0.5079  0.4953  0.5432  0.6949  0.6799  0.6705\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.3198  0.9266  0.7599  0.4114  0.5711  0.3179  0.9150  0.6536  0.5397  0.5727\n",
      " 0.0089  0.5409  0.0299  0.1370  0.7700  0.0098  0.8551  0.5623  0.5145  0.5064\n",
      " 0.9384  0.0718  0.6884  0.4205  0.1104  0.7483  0.3442  0.9000  0.8410  0.8821\n",
      " 0.3516  0.3865  0.6272  0.4937  0.9596  0.2711  0.5279  0.6794  0.3928  0.7076\n",
      " 0.1499  0.2423  0.3039  0.9808  0.6357  0.0980  0.2568  0.1525  0.8958  0.3557\n",
      " 0.6572  0.1140  0.1016  0.4064  0.9945  0.0815  0.7844  0.2270  0.1520  0.1577\n",
      " 0.0740  0.1414  0.3715  0.6994  0.2599  0.4900  0.4232  0.1233  0.3209  0.9867\n",
      " 0.9675  0.0663  0.3011  0.4342  0.7624  0.2496  0.5763  0.8607  0.2433  0.7928\n",
      " 0.7209  0.6852  0.0164  0.8413  0.7399  0.4050  0.4145  0.5156  0.0385  0.1101\n",
      " 0.7265  0.7870  0.4039  0.0515  0.4334  0.7130  0.7896  0.3651  0.0689  0.1405\n",
      " 0.7088 -0.0001  0.6092  0.8742  0.9791  0.3649  0.7401  0.2361  0.3385  0.3220\n",
      " 0.4713  0.3533  0.5761  0.9654  0.3404  0.1774  0.1213  0.9420  0.9760  0.6825\n",
      " 0.7458  0.5104  0.5598  0.7004  0.5278  0.7185  0.0057  0.8339  0.1406  0.2962\n",
      "[torch.FloatTensor of size 13x30]\n",
      "\n",
      "(4, 'losssssssssssssssssssss', 309.8980712890625)\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9760  0.5986  0.2211  0.8876  0.0281  0.0150  0.3405  0.3557  0.8210  0.7297\n",
      " 0.3258  0.4824  0.1471  0.5982  0.4437  0.9210  0.7388  0.8972  0.7576  0.4182\n",
      " 0.7806  0.8685  0.3995  0.5844  0.3544  0.6114  0.0218  0.6226  0.4438  0.6320\n",
      " 0.1724  0.8471  0.0743  0.2999  0.3790  0.0315  0.0350  0.1832  0.3220  0.0755\n",
      " 0.3628  0.3577  0.4847  0.0929  0.9416  0.2242  0.5833  0.6821  0.0763  0.4425\n",
      " 0.6435  0.3388  0.1241  0.2988  0.8147  0.4621  0.2771  0.2960  0.2026  0.7335\n",
      " 0.2742  0.7370  0.7098  0.5990  0.7346  0.9594  0.1518  0.6899  0.7690  0.9502\n",
      " 0.4833  0.4999  0.4520  0.4797  0.2149  0.4250  0.2529  0.6897  0.4798  0.1845\n",
      " 0.2271  0.7232  0.0637  0.4776  0.1179  0.8972  0.1468  0.9149  0.8057  0.9811\n",
      " 0.0107  0.0722  0.8531  0.5766  0.7573  0.2283  0.5655  0.7362  0.4536 -0.0010\n",
      " 0.3316  0.6291  0.2694  0.5554  0.9641  0.5843  0.1797  0.3017  0.8308  0.8936\n",
      " 0.0646  0.7928  0.4676  0.9484  0.2618  0.0165  0.3614  0.2896  0.7062  0.8024\n",
      " 0.0955  0.8726  0.2103  0.3426  0.1126  0.0294  0.0651  0.2348  0.6090  0.7058\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.1322  0.7564  0.2277  0.7828  0.5047  0.0905  0.6219  0.6334  0.1312  0.2169\n",
      " 0.1142  0.1525  0.6041  0.6153  0.2527  0.2099  0.2930  0.4330  0.7524  0.6720\n",
      " 0.1477  0.9820  0.5318  0.6909  0.5288  0.8959  0.7783  0.8294  0.1963  0.4980\n",
      " 0.2679  0.2133  0.1617  0.9173  0.2908  0.8948  0.1821  0.4408  0.7724  0.3174\n",
      " 0.2781  0.8070  0.3312  0.4526  0.7506  0.5731  0.5147  0.1527  0.4256  0.9761\n",
      " 0.7345  0.4360  0.1051  0.6144  0.3003  0.4794  0.7612  0.8496  0.4318  0.7527\n",
      " 0.0908  0.6507  0.4801  0.2227  0.8795  0.9220  0.1426  0.6760  0.6396  0.6715\n",
      " 0.8793  0.2269  0.3336  0.6699  0.0345  0.7940  0.3246  0.8731  0.0091  0.5983\n",
      " 0.0919  0.3210  0.9221  0.1332  0.7189  0.4181  0.2300  0.4263  0.5402  0.1076\n",
      " 0.2398  0.3437  0.5926  0.6225  0.6419  0.6270  0.5012  0.2773  0.2179  0.2294\n",
      " 0.5244  0.8890  0.7489  0.0696  0.1721  0.9483  0.4860  0.8718  0.1695  0.8911\n",
      " 0.7625  0.4311  0.4650  0.4603  0.8532  0.2721  0.7752  0.7284  0.8744  0.8319\n",
      " 0.3512  0.6713  0.5519  0.8726  0.5069  0.4943  0.5423  0.6939  0.6789  0.6695\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.3188  0.9256  0.7589  0.4106  0.5701  0.3177  0.9140  0.6526  0.5387  0.5717\n",
      " 0.0079  0.5399  0.0289  0.1360  0.7690  0.0108  0.8541  0.5613  0.5135  0.5054\n",
      " 0.9374  0.0708  0.6874  0.4195  0.1094  0.7473  0.3432  0.8990  0.8400  0.8811\n",
      " 0.3506  0.3855  0.6262  0.4927  0.9586  0.2701  0.5269  0.6784  0.3918  0.7066\n",
      " 0.1489  0.2413  0.3029  0.9798  0.6347  0.0970  0.2558  0.1515  0.8948  0.3547\n",
      " 0.6562  0.1130  0.1006  0.4054  0.9935  0.0805  0.7834  0.2260  0.1510  0.1567\n",
      " 0.0730  0.1404  0.3705  0.6984  0.2606  0.4890  0.4222  0.1223  0.3199  0.9857\n",
      " 0.9667  0.0653  0.3001  0.4332  0.7614  0.2486  0.5753  0.8597  0.2423  0.7918\n",
      " 0.7199  0.6842  0.0154  0.8403  0.7389  0.4040  0.4135  0.5146  0.0375  0.1091\n",
      " 0.7255  0.7860  0.4029  0.0525  0.4324  0.7120  0.7886  0.3641  0.0679  0.1395\n",
      " 0.7079 -0.0007  0.6083  0.8733  0.9782  0.3640  0.7391  0.2352  0.3376  0.3220\n",
      " 0.4703  0.3523  0.5751  0.9644  0.3394  0.1784  0.1203  0.9410  0.9750  0.6815\n",
      " 0.7448  0.5094  0.5588  0.6994  0.5268  0.7175  0.0047  0.8329  0.1396  0.2952\n",
      "[torch.FloatTensor of size 13x30]\n",
      "\n",
      "(5, 'losssssssssssssssssssss', 307.86541748046875)\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9770  0.5976  0.2201  0.8866  0.0289  0.0160  0.3396  0.3547  0.8200  0.7287\n",
      " 0.3268  0.4814  0.1461  0.5974  0.4431  0.9202  0.7378  0.8963  0.7566  0.4172\n",
      " 0.7816  0.8675  0.3985  0.5834  0.3545  0.6104  0.0222  0.6221  0.4428  0.6310\n",
      " 0.1734  0.8461  0.0746  0.2989  0.3786  0.0321  0.0341  0.1831  0.3210  0.0746\n",
      " 0.3638  0.3567  0.4837  0.0931  0.9407  0.2244  0.5825  0.6811  0.0769  0.4415\n",
      " 0.6445  0.3378  0.1234  0.2978  0.8137  0.4611  0.2780  0.2950  0.2016  0.7325\n",
      " 0.2752  0.7360  0.7090  0.5980  0.7339  0.9588  0.1508  0.6889  0.7680  0.9492\n",
      " 0.4843  0.4989  0.4510  0.4794  0.2139  0.4240  0.2535  0.6887  0.4788  0.1835\n",
      " 0.2281  0.7222  0.0627  0.4766  0.1169  0.8963  0.1472  0.9142  0.8047  0.9801\n",
      " 0.0117  0.0712  0.8521  0.5756  0.7563  0.2282  0.5651  0.7357  0.4526 -0.0014\n",
      " 0.3326  0.6281  0.2684  0.5547  0.9631  0.5840  0.1796  0.3015  0.8298  0.8926\n",
      " 0.0656  0.7918  0.4666  0.9477  0.2615  0.0155  0.3616  0.2886  0.7052  0.8014\n",
      " 0.0965  0.8716  0.2104  0.3423  0.1128  0.0298  0.0654  0.2338  0.6080  0.7048\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.1312  0.7554  0.2267  0.7818  0.5037  0.0896  0.6209  0.6324  0.1307  0.2162\n",
      " 0.1132  0.1515  0.6031  0.6143  0.2517  0.2089  0.2924  0.4321  0.7515  0.6710\n",
      " 0.1467  0.9811  0.5308  0.6899  0.5278  0.8949  0.7780  0.8284  0.1966  0.4970\n",
      " 0.2669  0.2123  0.1607  0.9163  0.2898  0.8938  0.1818  0.4399  0.7715  0.3166\n",
      " 0.2771  0.8060  0.3303  0.4516  0.7496  0.5722  0.5150  0.1529  0.4250  0.9751\n",
      " 0.7335  0.4350  0.1041  0.6134  0.2993  0.4784  0.7612  0.8487  0.4308  0.7517\n",
      " 0.0898  0.6497  0.4791  0.2217  0.8785  0.9210  0.1420  0.6750  0.6386  0.6705\n",
      " 0.8783  0.2259  0.3326  0.6689  0.0335  0.7930  0.3247  0.8724  0.0082  0.5973\n",
      " 0.0909  0.3200  0.9211  0.1322  0.7179  0.4171  0.2302  0.4253  0.5392  0.1066\n",
      " 0.2388  0.3427  0.5916  0.6215  0.6409  0.6260  0.5003  0.2763  0.2171  0.2284\n",
      " 0.5234  0.8880  0.7479  0.0686  0.1711  0.9473  0.4854  0.8711  0.1687  0.8901\n",
      " 0.7615  0.4301  0.4640  0.4593  0.8522  0.2711  0.7742  0.7274  0.8734  0.8309\n",
      " 0.3503  0.6703  0.5509  0.8716  0.5059  0.4933  0.5414  0.6929  0.6779  0.6685\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.3177  0.9246  0.7579  0.4098  0.5690  0.3176  0.9130  0.6516  0.5377  0.5708\n",
      " 0.0069  0.5389  0.0279  0.1350  0.7680  0.0118  0.8531  0.5603  0.5125  0.5044\n",
      " 0.9364  0.0698  0.6864  0.4185  0.1084  0.7463  0.3422  0.8980  0.8390  0.8801\n",
      " 0.3496  0.3845  0.6252  0.4917  0.9576  0.2691  0.5259  0.6774  0.3908  0.7056\n",
      " 0.1479  0.2403  0.3019  0.9788  0.6337  0.0960  0.2548  0.1505  0.8938  0.3537\n",
      " 0.6552  0.1120  0.0996  0.4044  0.9925  0.0795  0.7824  0.2250  0.1500  0.1557\n",
      " 0.0720  0.1394  0.3695  0.6974  0.2613  0.4880  0.4212  0.1213  0.3189  0.9847\n",
      " 0.9658  0.0644  0.2991  0.4322  0.7604  0.2476  0.5743  0.8587  0.2413  0.7908\n",
      " 0.7189  0.6832  0.0144  0.8393  0.7379  0.4030  0.4125  0.5136  0.0365  0.1081\n",
      " 0.7245  0.7850  0.4019  0.0535  0.4314  0.7110  0.7876  0.3631  0.0669  0.1385\n",
      " 0.7071 -0.0013  0.6074  0.8723  0.9773  0.3631  0.7383  0.2344  0.3367  0.3218\n",
      " 0.4693  0.3513  0.5741  0.9634  0.3384  0.1794  0.1193  0.9400  0.9740  0.6805\n",
      " 0.7438  0.5084  0.5578  0.6984  0.5258  0.7165  0.0038  0.8319  0.1386  0.2942\n",
      "[torch.FloatTensor of size 13x30]\n",
      "\n",
      "(6, 'losssssssssssssssssssss', 308.191162109375)\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9780  0.5966  0.2191  0.8856  0.0297  0.0170  0.3386  0.3538  0.8190  0.7278\n",
      " 0.3278  0.4804  0.1451  0.5969  0.4425  0.9194  0.7368  0.8955  0.7556  0.4162\n",
      " 0.7826  0.8665  0.3975  0.5824  0.3545  0.6096  0.0226  0.6214  0.4418  0.6300\n",
      " 0.1744  0.8451  0.0748  0.2986  0.3781  0.0326  0.0331  0.1830  0.3200  0.0736\n",
      " 0.3648  0.3557  0.4827  0.0933  0.9398  0.2244  0.5816  0.6801  0.0776  0.4405\n",
      " 0.6455  0.3368  0.1226  0.2969  0.8127  0.4601  0.2789  0.2940  0.2006  0.7315\n",
      " 0.2762  0.7350  0.7084  0.5970  0.7331  0.9581  0.1498  0.6879  0.7670  0.9482\n",
      " 0.4853  0.4979  0.4500  0.4790  0.2129  0.4234  0.2540  0.6877  0.4778  0.1825\n",
      " 0.2291  0.7212  0.0618  0.4756  0.1159  0.8954  0.1479  0.9134  0.8037  0.9791\n",
      " 0.0127  0.0702  0.8511  0.5746  0.7553  0.2279  0.5650  0.7350  0.4516 -0.0016\n",
      " 0.3336  0.6271  0.2674  0.5540  0.9621  0.5835  0.1799  0.3012  0.8288  0.8916\n",
      " 0.0666  0.7908  0.4656  0.9470  0.2611  0.0146  0.3615  0.2884  0.7042  0.8004\n",
      " 0.0975  0.8706  0.2105  0.3419  0.1128  0.0302  0.0656  0.2336  0.6070  0.7038\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.1302  0.7544  0.2257  0.7808  0.5027  0.0886  0.6199  0.6315  0.1301  0.2154\n",
      " 0.1122  0.1505  0.6021  0.6133  0.2507  0.2080  0.2918  0.4312  0.7505  0.6700\n",
      " 0.1457  0.9801  0.5298  0.6889  0.5268  0.8939  0.7776  0.8274  0.1968  0.4964\n",
      " 0.2659  0.2113  0.1597  0.9153  0.2888  0.8928  0.1812  0.4390  0.7705  0.3156\n",
      " 0.2761  0.8050  0.3292  0.4506  0.7486  0.5711  0.5154  0.1529  0.4244  0.9741\n",
      " 0.7325  0.4340  0.1031  0.6124  0.2983  0.4774  0.7608  0.8479  0.4303  0.7507\n",
      " 0.0888  0.6487  0.4781  0.2207  0.8775  0.9200  0.1416  0.6740  0.6376  0.6695\n",
      " 0.8773  0.2249  0.3316  0.6679  0.0325  0.7920  0.3250  0.8716  0.0072  0.5963\n",
      " 0.0899  0.3190  0.9201  0.1312  0.7169  0.4161  0.2301  0.4244  0.5382  0.1056\n",
      " 0.2378  0.3417  0.5906  0.6205  0.6399  0.6250  0.4994  0.2754  0.2162  0.2275\n",
      " 0.5224  0.8870  0.7469  0.0676  0.1701  0.9463  0.4847  0.8704  0.1678  0.8892\n",
      " 0.7605  0.4291  0.4630  0.4583  0.8512  0.2701  0.7732  0.7264  0.8724  0.8299\n",
      " 0.3493  0.6693  0.5499  0.8706  0.5049  0.4923  0.5404  0.6920  0.6770  0.6675\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.3167  0.9236  0.7569  0.4091  0.5680  0.3175  0.9120  0.6506  0.5367  0.5698\n",
      " 0.0060  0.5379  0.0270  0.1340  0.7670  0.0128  0.8521  0.5593  0.5115  0.5034\n",
      " 0.9354  0.0688  0.6854  0.4175  0.1074  0.7453  0.3412  0.8970  0.8380  0.8791\n",
      " 0.3487  0.3835  0.6243  0.4907  0.9566  0.2681  0.5249  0.6764  0.3898  0.7046\n",
      " 0.1469  0.2393  0.3009  0.9778  0.6327  0.0950  0.2538  0.1495  0.8928  0.3527\n",
      " 0.6542  0.1110  0.0986  0.4034  0.9915  0.0785  0.7814  0.2240  0.1490  0.1547\n",
      " 0.0709  0.1383  0.3685  0.6964  0.2618  0.4870  0.4203  0.1203  0.3179  0.9837\n",
      " 0.9649  0.0634  0.2981  0.4312  0.7594  0.2466  0.5733  0.8577  0.2404  0.7898\n",
      " 0.7179  0.6822  0.0134  0.8383  0.7369  0.4020  0.4115  0.5126  0.0355  0.1071\n",
      " 0.7235  0.7840  0.4009  0.0545  0.4304  0.7100  0.7866  0.3621  0.0659  0.1375\n",
      " 0.7062 -0.0016  0.6065  0.8714  0.9765  0.3622  0.7374  0.2335  0.3359  0.3215\n",
      " 0.4683  0.3503  0.5731  0.9624  0.3374  0.1803  0.1183  0.9390  0.9730  0.6795\n",
      " 0.7428  0.5074  0.5568  0.6974  0.5248  0.7155  0.0029  0.8309  0.1376  0.2932\n",
      "[torch.FloatTensor of size 13x30]\n",
      "\n",
      "(7, 'losssssssssssssssssssss', 304.91094970703125)\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9789  0.5956  0.2181  0.8846  0.0305  0.0180  0.3376  0.3528  0.8180  0.7268\n",
      " 0.3288  0.4794  0.1441  0.5964  0.4418  0.9185  0.7358  0.8947  0.7546  0.4152\n",
      " 0.7836  0.8655  0.3965  0.5814  0.3543  0.6089  0.0229  0.6210  0.4408  0.6290\n",
      " 0.1754  0.8441  0.0750  0.2982  0.3776  0.0330  0.0335  0.1827  0.3190  0.0726\n",
      " 0.3658  0.3547  0.4817  0.0934  0.9389  0.2242  0.5808  0.6791  0.0784  0.4395\n",
      " 0.6465  0.3358  0.1218  0.2959  0.8117  0.4591  0.2799  0.2930  0.1996  0.7305\n",
      " 0.2772  0.7340  0.7077  0.5960  0.7322  0.9575  0.1488  0.6869  0.7660  0.9472\n",
      " 0.4863  0.4969  0.4490  0.4785  0.2119  0.4232  0.2544  0.6867  0.4768  0.1815\n",
      " 0.2301  0.7202  0.0621  0.4746  0.1149  0.8945  0.1484  0.9125  0.8027  0.9781\n",
      " 0.0137  0.0692  0.8501  0.5736  0.7545  0.2276  0.5646  0.7343  0.4506 -0.0016\n",
      " 0.3346  0.6261  0.2664  0.5532  0.9611  0.5833  0.1801  0.3007  0.8278  0.8906\n",
      " 0.0676  0.7898  0.4646  0.9463  0.2607  0.0136  0.3614  0.2880  0.7033  0.7994\n",
      " 0.0985  0.8696  0.2104  0.3414  0.1132  0.0304  0.0658  0.2334  0.6060  0.7028\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.1292  0.7535  0.2247  0.7798  0.5018  0.0876  0.6190  0.6306  0.1295  0.2146\n",
      " 0.1112  0.1496  0.6011  0.6123  0.2497  0.2070  0.2912  0.4302  0.7495  0.6690\n",
      " 0.1447  0.9791  0.5288  0.6879  0.5258  0.8929  0.7773  0.8264  0.1969  0.4957\n",
      " 0.2649  0.2103  0.1587  0.9143  0.2878  0.8918  0.1806  0.4381  0.7695  0.3147\n",
      " 0.2750  0.8039  0.3282  0.4495  0.7476  0.5701  0.5155  0.1532  0.4237  0.9731\n",
      " 0.7315  0.4330  0.1021  0.6114  0.2973  0.4764  0.7606  0.8469  0.4297  0.7497\n",
      " 0.0878  0.6477  0.4771  0.2197  0.8765  0.9190  0.1415  0.6730  0.6366  0.6685\n",
      " 0.8763  0.2239  0.3306  0.6670  0.0316  0.7910  0.3252  0.8707  0.0063  0.5956\n",
      " 0.0889  0.3180  0.9191  0.1302  0.7159  0.4151  0.2302  0.4235  0.5372  0.1046\n",
      " 0.2368  0.3407  0.5896  0.6195  0.6389  0.6240  0.4984  0.2744  0.2154  0.2265\n",
      " 0.5214  0.8860  0.7459  0.0666  0.1691  0.9453  0.4840  0.8699  0.1670  0.8883\n",
      " 0.7595  0.4281  0.4620  0.4573  0.8502  0.2691  0.7722  0.7254  0.8714  0.8289\n",
      " 0.3483  0.6683  0.5489  0.8696  0.5039  0.4913  0.5395  0.6910  0.6760  0.6665\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.3157  0.9226  0.7559  0.4083  0.5670  0.3174  0.9110  0.6496  0.5356  0.5688\n",
      " 0.0051  0.5369  0.0260  0.1330  0.7660  0.0138  0.8511  0.5583  0.5105  0.5024\n",
      " 0.9344  0.0678  0.6844  0.4165  0.1064  0.7443  0.3402  0.8960  0.8370  0.8781\n",
      " 0.3478  0.3825  0.6233  0.4897  0.9556  0.2671  0.5239  0.6754  0.3888  0.7036\n",
      " 0.1459  0.2383  0.2999  0.9768  0.6317  0.0940  0.2528  0.1485  0.8918  0.3517\n",
      " 0.6532  0.1100  0.0976  0.4025  0.9905  0.0775  0.7804  0.2230  0.1480  0.1537\n",
      " 0.0699  0.1373  0.3674  0.6954  0.2622  0.4860  0.4193  0.1193  0.3169  0.9827\n",
      " 0.9640  0.0624  0.2971  0.4302  0.7584  0.2456  0.5723  0.8567  0.2394  0.7888\n",
      " 0.7169  0.6812  0.0125  0.8373  0.7359  0.4010  0.4105  0.5116  0.0346  0.1061\n",
      " 0.7225  0.7830  0.3999  0.0555  0.4294  0.7090  0.7856  0.3611  0.0649  0.1365\n",
      " 0.7054 -0.0019  0.6057  0.8705  0.9756  0.3613  0.7366  0.2327  0.3350  0.3212\n",
      " 0.4673  0.3493  0.5721  0.9614  0.3364  0.1813  0.1173  0.9380  0.9720  0.6785\n",
      " 0.7418  0.5064  0.5558  0.6964  0.5238  0.7145  0.0020  0.8299  0.1366  0.2922\n",
      "[torch.FloatTensor of size 13x30]\n",
      "\n",
      "(8, 'losssssssssssssssssssss', 307.7912902832031)\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9799  0.5946  0.2171  0.8836  0.0312  0.0190  0.3366  0.3518  0.8170  0.7258\n",
      " 0.3298  0.4784  0.1432  0.5957  0.4411  0.9177  0.7348  0.8939  0.7536  0.4142\n",
      " 0.7846  0.8645  0.3955  0.5807  0.3540  0.6081  0.0232  0.6205  0.4398  0.6280\n",
      " 0.1764  0.8431  0.0751  0.2978  0.3769  0.0334  0.0341  0.1824  0.3180  0.0716\n",
      " 0.3667  0.3537  0.4806  0.0937  0.9379  0.2238  0.5800  0.6781  0.0792  0.4385\n",
      " 0.6475  0.3348  0.1210  0.2949  0.8108  0.4581  0.2808  0.2920  0.1986  0.7295\n",
      " 0.2782  0.7330  0.7069  0.5950  0.7315  0.9568  0.1478  0.6859  0.7650  0.9462\n",
      " 0.4873  0.4959  0.4480  0.4778  0.2109  0.4229  0.2549  0.6857  0.4758  0.1805\n",
      " 0.2311  0.7192  0.0626  0.4736  0.1139  0.8935  0.1488  0.9117  0.8017  0.9771\n",
      " 0.0147  0.0682  0.8491  0.5726  0.7536  0.2272  0.5642  0.7338  0.4496 -0.0014\n",
      " 0.3355  0.6251  0.2654  0.5524  0.9601  0.5829  0.1805  0.3002  0.8268  0.8896\n",
      " 0.0686  0.7888  0.4636  0.9456  0.2602  0.0127  0.3612  0.2880  0.7023  0.7984\n",
      " 0.0995  0.8686  0.2102  0.3408  0.1134  0.0310  0.0659  0.2331  0.6050  0.7018\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.1282  0.7525  0.2237  0.7788  0.5008  0.0866  0.6181  0.6296  0.1288  0.2137\n",
      " 0.1102  0.1486  0.6001  0.6113  0.2487  0.2060  0.2904  0.4293  0.7486  0.6680\n",
      " 0.1437  0.9781  0.5279  0.6869  0.5248  0.8919  0.7770  0.8254  0.1969  0.4950\n",
      " 0.2639  0.2093  0.1577  0.9133  0.2868  0.8908  0.1803  0.4371  0.7685  0.3137\n",
      " 0.2740  0.8029  0.3272  0.4485  0.7466  0.5691  0.5154  0.1537  0.4230  0.9721\n",
      " 0.7305  0.4320  0.1011  0.6104  0.2963  0.4754  0.7606  0.8460  0.4290  0.7487\n",
      " 0.0868  0.6467  0.4761  0.2187  0.8755  0.9180  0.1412  0.6720  0.6356  0.6675\n",
      " 0.8753  0.2229  0.3296  0.6660  0.0306  0.7900  0.3253  0.8700  0.0054  0.5948\n",
      " 0.0879  0.3170  0.9181  0.1292  0.7149  0.4141  0.2303  0.4225  0.5362  0.1036\n",
      " 0.2358  0.3397  0.5886  0.6185  0.6379  0.6230  0.4975  0.2734  0.2147  0.2255\n",
      " 0.5204  0.8850  0.7449  0.0656  0.1681  0.9443  0.4833  0.8693  0.1662  0.8875\n",
      " 0.7585  0.4271  0.4610  0.4563  0.8492  0.2681  0.7713  0.7245  0.8704  0.8279\n",
      " 0.3473  0.6673  0.5479  0.8686  0.5029  0.4903  0.5386  0.6900  0.6750  0.6655\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.3147  0.9216  0.7549  0.4075  0.5660  0.3174  0.9100  0.6486  0.5346  0.5678\n",
      " 0.0042  0.5359  0.0250  0.1320  0.7650  0.0148  0.8501  0.5573  0.5095  0.5014\n",
      " 0.9334  0.0668  0.6834  0.4155  0.1054  0.7433  0.3392  0.8950  0.8360  0.8771\n",
      " 0.3469  0.3815  0.6223  0.4887  0.9546  0.2661  0.5229  0.6744  0.3878  0.7026\n",
      " 0.1449  0.2373  0.2989  0.9758  0.6307  0.0930  0.2518  0.1475  0.8908  0.3507\n",
      " 0.6522  0.1090  0.0966  0.4015  0.9895  0.0766  0.7794  0.2220  0.1470  0.1527\n",
      " 0.0689  0.1363  0.3664  0.6943  0.2625  0.4849  0.4182  0.1183  0.3159  0.9817\n",
      " 0.9631  0.0614  0.2961  0.4292  0.7574  0.2446  0.5713  0.8557  0.2384  0.7878\n",
      " 0.7160  0.6802  0.0115  0.8363  0.7349  0.4000  0.4095  0.5106  0.0336  0.1051\n",
      " 0.7215  0.7820  0.3989  0.0565  0.4284  0.7080  0.7846  0.3601  0.0639  0.1355\n",
      " 0.7047 -0.0021  0.6049  0.8697  0.9748  0.3604  0.7358  0.2319  0.3342  0.3207\n",
      " 0.4663  0.3483  0.5711  0.9604  0.3354  0.1823  0.1163  0.9370  0.9710  0.6775\n",
      " 0.7408  0.5054  0.5548  0.6954  0.5228  0.7135  0.0012  0.8289  0.1356  0.2912\n",
      "[torch.FloatTensor of size 13x30]\n",
      "\n",
      "(9, 'losssssssssssssssssssss', 305.9522399902344)\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.9809  0.5936  0.2161  0.8826  0.0320  0.0201  0.3356  0.3509  0.8160  0.7248\n",
      " 0.3308  0.4774  0.1422  0.5950  0.4407  0.9169  0.7338  0.8931  0.7526  0.4132\n",
      " 0.7856  0.8635  0.3945  0.5799  0.3536  0.6072  0.0234  0.6201  0.4388  0.6270\n",
      " 0.1774  0.8421  0.0751  0.2977  0.3762  0.0337  0.0346  0.1821  0.3170  0.0706\n",
      " 0.3677  0.3528  0.4796  0.0938  0.9370  0.2233  0.5791  0.6770  0.0801  0.4374\n",
      " 0.6485  0.3338  0.1203  0.2938  0.8098  0.4571  0.2817  0.2910  0.1976  0.7285\n",
      " 0.2792  0.7320  0.7061  0.5940  0.7308  0.9562  0.1468  0.6849  0.7640  0.9452\n",
      " 0.4883  0.4949  0.4470  0.4772  0.2099  0.4229  0.2552  0.6847  0.4748  0.1795\n",
      " 0.2321  0.7182  0.0631  0.4726  0.1129  0.8925  0.1493  0.9108  0.8007  0.9761\n",
      " 0.0157  0.0673  0.8481  0.5716  0.7528  0.2267  0.5640  0.7331  0.4486 -0.0011\n",
      " 0.3365  0.6242  0.2644  0.5515  0.9591  0.5825  0.1810  0.2997  0.8258  0.8886\n",
      " 0.0696  0.7878  0.4626  0.9448  0.2596  0.0117  0.3608  0.2883  0.7013  0.7974\n",
      " 0.1005  0.8676  0.2100  0.3406  0.1136  0.0314  0.0659  0.2327  0.6040  0.7008\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.1272  0.7515  0.2227  0.7778  0.4998  0.0856  0.6171  0.6286  0.1280  0.2131\n",
      " 0.1092  0.1476  0.5991  0.6103  0.2477  0.2050  0.2896  0.4283  0.7477  0.6671\n",
      " 0.1427  0.9771  0.5269  0.6859  0.5238  0.8909  0.7766  0.8244  0.1972  0.4942\n",
      " 0.2630  0.2083  0.1567  0.9123  0.2858  0.8898  0.1798  0.4362  0.7675  0.3128\n",
      " 0.2730  0.8019  0.3262  0.4475  0.7455  0.5681  0.5155  0.1541  0.4222  0.9711\n",
      " 0.7295  0.4310  0.1001  0.6094  0.2953  0.4744  0.7606  0.8451  0.4283  0.7477\n",
      " 0.0858  0.6457  0.4751  0.2177  0.8745  0.9170  0.1410  0.6710  0.6346  0.6665\n",
      " 0.8743  0.2219  0.3286  0.6650  0.0296  0.7890  0.3252  0.8694  0.0045  0.5940\n",
      " 0.0869  0.3160  0.9171  0.1282  0.7139  0.4131  0.2305  0.4215  0.5352  0.1026\n",
      " 0.2348  0.3387  0.5876  0.6175  0.6369  0.6220  0.4965  0.2725  0.2139  0.2245\n",
      " 0.5194  0.8840  0.7439  0.0646  0.1672  0.9433  0.4827  0.8687  0.1654  0.8866\n",
      " 0.7575  0.4261  0.4600  0.4553  0.8482  0.2671  0.7703  0.7235  0.8694  0.8269\n",
      " 0.3463  0.6663  0.5469  0.8676  0.5019  0.4893  0.5376  0.6890  0.6741  0.6645\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.3137  0.9206  0.7539  0.4067  0.5650  0.3171  0.9090  0.6476  0.5337  0.5669\n",
      " 0.0033  0.5349  0.0240  0.1310  0.7640  0.0157  0.8491  0.5563  0.5085  0.5004\n",
      " 0.9324  0.0658  0.6824  0.4145  0.1044  0.7423  0.3382  0.8940  0.8350  0.8762\n",
      " 0.3459  0.3806  0.6213  0.4877  0.9536  0.2651  0.5219  0.6734  0.3868  0.7016\n",
      " 0.1439  0.2363  0.2979  0.9748  0.6297  0.0920  0.2508  0.1466  0.8898  0.3497\n",
      " 0.6512  0.1080  0.0956  0.4005  0.9885  0.0756  0.7784  0.2210  0.1460  0.1518\n",
      " 0.0679  0.1353  0.3654  0.6933  0.2624  0.4839  0.4172  0.1172  0.3148  0.9807\n",
      " 0.9621  0.0604  0.2951  0.4282  0.7564  0.2436  0.5703  0.8547  0.2374  0.7868\n",
      " 0.7150  0.6792  0.0106  0.8353  0.7339  0.3990  0.4085  0.5096  0.0326  0.1041\n",
      " 0.7205  0.7810  0.3979  0.0575  0.4274  0.7070  0.7836  0.3591  0.0629  0.1345\n",
      " 0.7042 -0.0022  0.6041  0.8688  0.9740  0.3596  0.7349  0.2310  0.3334  0.3203\n",
      " 0.4652  0.3473  0.5701  0.9594  0.3344  0.1832  0.1153  0.9360  0.9700  0.6765\n",
      " 0.7398  0.5044  0.5538  0.6945  0.5218  0.7125  0.0004  0.8279  0.1346  0.2902\n",
      "[torch.FloatTensor of size 13x30]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_iters = 200\n",
    "learning_rate = .001\n",
    "steps = 2\n",
    "hidden_size_decoder = 500\n",
    "num_rules = 2\n",
    "\n",
    "K = 34 ##For top K\n",
    "\n",
    "#Core Relations:\n",
    "## Tree of animals---6 ground facts:\n",
    "#canary, eagle are birds. shark salmon are fishs. fishs,birds are animals\n",
    "## 1 has_is per object---7 ground facts:\n",
    "## animal breathes, bird flies, fish swims, canary sings, eagle claws, shark bites, salmon pink\n",
    "\n",
    "core_rel = Variable(torch.rand(13, num_feat_facts), requires_grad=True)\n",
    "\n",
    "#rules = Variable(torch.rand(num_rules, num_predicates*3), requires_grad=True)\n",
    "\n",
    "\n",
    "#embeddings = Variable(torch.eye(num_predicates), requires_grad=True)\n",
    "#embeddings = Variable(torch.rand(num_predicates, num_feat), requires_grad=True)\n",
    "rule1 = torch.Tensor([1,0,1,0,1,0]).view(1,-1)\n",
    "rule2 = torch.Tensor([0,1,1,0,0,1]).view(1,-1)\n",
    "rules = Variable(torch.cat((rule1,rule2),0), requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "        #{'params': [rules]},\n",
    "        {'params': [core_rel]}\n",
    "    ], lr = learning_rate)\n",
    "\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "\n",
    "for epoch in range(num_iters):\n",
    "    optimizer.zero_grad()\n",
    "    facts = torch.cat((core_rel, Variable(torch.ones(13).view(13,1))), 1)\n",
    "    #print('epoch {} before_decoder'.format(epoch), facts)\n",
    "        \n",
    "    for step in range(steps):\n",
    "        facts = forward_step(facts)\n",
    "        #if epoch % 39 == 0 and epoch>0 :\n",
    "            #print('epoch {}, step {}'.format(epoch,step+1), valuation)\n",
    "    \n",
    "    \n",
    "    loss = criterion(facts[:,:-1], knowledge_pos)\n",
    "    print(epoch, 'losssssssssssssssssssss',loss.data[0])\n",
    "    #pdb.set_trace()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(core_rel)\n",
    "\n",
    "#data[0,6,0],data[1,6,0],data[1,6,2],data[1,6,6] = 0,0,0,0\n",
    "# Knows: salmon is fish, salmon is salmon\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(embeddings)\n",
    "print(rules)\n",
    "\n",
    "rules_aux = torch.cat((rules[:,:num_feat],rules[:,num_feat:2*num_feat],rules[:,2*num_feat:3*num_feat]),0)\n",
    "rules_aux = rules_aux.repeat(num_predicates,1)\n",
    "embeddings_aux = embeddings.repeat(1,num_rules*3).view(-1,num_feat)\n",
    "\n",
    "\n",
    "unifs_real = F.cosine_similarity(embeddings_aux, rules_aux).view(num_predicates,-1)\n",
    "print('aaaaa',unifs_real)\n",
    "#print('oooo',F.pairwise_distance(embeddings_aux, rules_aux).view(num_predicates,-1))\n",
    "\n",
    "unifs = F.pairwise_distance(embeddings_aux, rules_aux).view(num_predicates,-1)\n",
    "unifs = torch.exp(-unifs)\n",
    "#print(unifs)\n",
    "unifs_sum = torch.sum(unifs, 0)\n",
    "unifs= unifs/unifs_sum\n",
    "print('finaaaal',unifs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rules[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decoder_efficient(valuation):\n",
    "\n",
    "    ##Unifications\n",
    "    rules_aux = torch.cat((rules[:,:num_feat],rules[:,num_feat:2*num_feat],rules[:,2*num_feat:3*num_feat]),0)\n",
    "    rules_aux = rules_aux.repeat(num_predicates,1)\n",
    "    embeddings_aux = embeddings.repeat(1,num_rules*3).view(-1,num_feat)\n",
    "    #embeddings_aux = F.dropout(embeddings_aux, p=.1,training=True)\n",
    "    #rules_aux = F.dropout(rules_aux, p=.1, training=True)\n",
    "\n",
    "    unifs = F.cosine_similarity(embeddings_aux, rules_aux).view(num_predicates,-1)\n",
    "    \n",
    "    #unifs = F.pairwise_distance(embeddings_aux, rules_aux).view(num_predicates,-1)\n",
    "    #unifs = torch.exp(-unifs)\n",
    "    #unifs_sum = torch.sum(unifs, 0)\n",
    "    #unifs= unifs/unifs_sum\n",
    "    \n",
    "    #unifs=Variable(torch.Tensor([[1,0,1,1,1,0],[0,1,0,0,0,1]]))\n",
    "\n",
    "\n",
    "    valuation_new = Variable(torch.Tensor(valuation.size()))\n",
    "    for predicate in intensional_predicates:\n",
    "        for s in range(num_subjects):\n",
    "            for o in range(num_objects):\n",
    "                valuation_aux = Variable(torch.Tensor([0]))\n",
    "                for body1 in range(num_predicates):\n",
    "                    for body2 in range(num_predicates):\n",
    "                        num = torch.min(valuation[body1][s,:],valuation[body2][:,o])\n",
    "                        num = torch.max(num)\n",
    "\n",
    "                        ## max across three rules\n",
    "                        new = Variable(torch.Tensor([0]))\n",
    "                        for rule in range(num_rules): \n",
    "                            unif = unifs[predicate][rule]*unifs[body1][num_rules+rule]*unifs[body2][2*num_rules+rule]\n",
    "                            new = torch.max(new,unif)\n",
    "                            #could be amalgamate\n",
    "\n",
    "                        num = num*new \n",
    "                        \n",
    "                        valuation_aux = amalgamate(valuation_aux, num)\n",
    "                        \n",
    "                        #if predicate == 0 and s==1 and o==1 and valuation_aux.data[0]>.3:\n",
    "                        #    print('body1', body1, 'body2', body2)\n",
    "\n",
    "                valuation_new[predicate,s,o] = amalgamate(valuation[predicate,s,o], valuation_aux)\n",
    "\n",
    "    return valuation_new\n",
    "    \n",
    "def amalgamate(x,y):\n",
    "    return x + y - x*y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
